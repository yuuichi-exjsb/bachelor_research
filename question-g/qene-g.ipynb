{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 626/626 [00:00<00:00, 42.1kB/s]\n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 1.72MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 9976.58 MB. The target location /home/forte/.cache/huggingface/hub only has 2.15 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 9976.58 MB. The target location /home/forte/.cache/huggingface/hub/models--tifa-benchmark--llama2_tifa_question_generation/blobs only has 2.15 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 10.5M/9.98G [00:00<02:17, 72.3MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 9976.58 MB. The target location /home/forte/.cache/huggingface/hub only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 9976.58 MB. The target location /home/forte/.cache/huggingface/hub/models--tifa-benchmark--llama2_tifa_question_generation/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 10.5M/9.98G [00:00<02:27, 67.4MB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model tifa-benchmark/llama2_tifa_question_generation with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 565, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3020, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1040, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 429, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1431, in hf_hub_download\n    http_get(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 554, in http_get\n    temp_file.write(chunk)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/tempfile.py\", line 474, in func_wrapper\n    return func(*args, **kwargs)\nOSError: [Errno 28] No space left on device\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3020, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1040, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 429, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1431, in hf_hub_download\n    http_get(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 554, in http_get\n    temp_file.write(chunk)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/tempfile.py\", line 474, in func_wrapper\n    return func(*args, **kwargs)\nOSError: [Errno 28] No space left on device\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# prepare the LLaMA 2 model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtifa-benchmark/llama2_tifa_question_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# formating prompt following LLaMA 2 style\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_qg_prompt\u001b[39m(caption):\n",
      "File \u001b[0;32m~/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/pipelines/__init__.py:868\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 868\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    879\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/pipelines/base.py:282\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    281\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model tifa-benchmark/llama2_tifa_question_generation with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 565, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3020, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1040, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 429, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1431, in hf_hub_download\n    http_get(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 554, in http_get\n    temp_file.write(chunk)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/tempfile.py\", line 474, in func_wrapper\n    return func(*args, **kwargs)\nOSError: [Errno 28] No space left on device\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3020, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1040, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/transformers/utils/hub.py\", line 429, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1431, in hf_hub_download\n    http_get(\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 554, in http_get\n    temp_file.write(chunk)\n  File \"/home/forte/anaconda3/envs/nishida/lib/python3.8/tempfile.py\", line 474, in func_wrapper\n    return func(*args, **kwargs)\nOSError: [Errno 28] No space left on device\n\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "# prepare the LLaMA 2 model\n",
    "model_name = \"tifa-benchmark/llama2_tifa_question_generation\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "# formating prompt following LLaMA 2 style\n",
    "def create_qg_prompt(caption):\n",
    "    INTRO_BLURB = \"Given an image description, generate one or two multiple-choice questions that verifies if the image description is correct.\\nClassify each concept into a type (object, human, animal, food, activity, attribute, counting, color, material, spatial, location, shape, other), and then generate a question for each type.\\n\"\n",
    "    formated_prompt = f\"<s>[INST] <<SYS>>\\n{INTRO_BLURB}\\n<</SYS>>\\n\\n\"\n",
    "    formated_prompt += f\"Description: {caption} [/INST] Entities:\"\n",
    "    return formated_prompt\n",
    "\n",
    "\n",
    "test_caption = \"a blue rabbit and a red plane\"\n",
    "\n",
    "# create prompt\n",
    "prompt = create_qg_prompt(test_caption)\n",
    "\n",
    "# text completion\n",
    "sequences = pipeline(\n",
    "        prompt, do_sample=False, num_beams=5, num_return_sequences=1, max_length=512)\n",
    "output = sequences[0]['generated_text'][len(prompt):]\n",
    "output = output.split('\\n\\n')[0]\n",
    "\n",
    "# output\n",
    "print(output)\n",
    "\n",
    "#### Expected output ###\n",
    "#  rabbit, plane\n",
    "# Activites:\n",
    "# Colors: blue, red\n",
    "# Counting:\n",
    "# Other attributes:\n",
    "# About rabbit (animal):\n",
    "# Q: is this a rabbit?\n",
    "# Choices: yes, no\n",
    "# A: yes\n",
    "# About rabbit (animal):\n",
    "# Q: what animal is in the picture?\n",
    "# Choices: rabbit, dog, cat, fish\n",
    "# A: rabbit\n",
    "# About plane (object):\n",
    "# Q: is this a plane?\n",
    "# Choices: yes, no\n",
    "# A: yes\n",
    "# About plane (object):\n",
    "# Q: what type of vehicle is this?\n",
    "# Choices: plane, car, motorcycle, bus\n",
    "# A: plane\n",
    "# About blue (color):\n",
    "# Q: is the rabbit blue?\n",
    "# Choices: yes, no\n",
    "# A: yes\n",
    "# About blue (color):\n",
    "# Q: what color is the rabbit?\n",
    "# Choices: blue, red, yellow, green\n",
    "# A: blue\n",
    "# About red (color):\n",
    "# Q: is the plane red?\n",
    "# Choices: yes, no\n",
    "# A: yes\n",
    "# About red (color):\n",
    "# Q: what color is the plane?\n",
    "# Choices: red, blue, yellow, green\n",
    "# A: red\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nishida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
